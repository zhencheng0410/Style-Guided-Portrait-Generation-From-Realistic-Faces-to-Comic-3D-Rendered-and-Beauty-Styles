{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680ff3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "#為 train Data 建立 Dataset 讀取方式 \n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_in_dir, img_label_dir, transform=None): \n",
    "        self.img_in_dir = img_in_dir\n",
    "        self.img_label_dir = img_label_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(img_in_dir) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_in_path = os.path.join(self.img_in_dir, self.images[idx]) \n",
    "        img_label_path = os.path.join(self.img_label_dir, self.images[idx])\n",
    "        image_in = Image.open(img_in_path).convert(\"RGB\") \n",
    "        image_label = Image.open(img_label_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image_in = self.transform(image_in) \n",
    "            image_label = self.transform(image_label)\n",
    "        return image_in, image_label\n",
    "        \n",
    "img_input = './dataset/3D Rendered Cartoon Style/train/images' \n",
    "img_label = './dataset/3D Rendered Cartoon Style/train/labels'\n",
    "transform = transforms.Compose([\n",
    "     transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "trainset = CustomImageDataset(img_in_dir=img_input, img_label_dir=img_label, transform=transform)\n",
    "# 建立 trainloader\n",
    "trainloader = DataLoader(trainset, batch_size=16, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b192b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_styles=3):\n",
    "        super(UNet, self).__init__()\n",
    "        self.num_styles = num_styles\n",
    "\n",
    "        # === Encoder ===\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(3 + num_styles, 64, kernel_size=3, padding=1),  # 3 RGB + style one-hot\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # === Decoder ===\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, style_id):\n",
    "        # style_id: tensor of shape [B] or [B, 1] with values 0, 1, or 2\n",
    "        # Convert style to one-hot: [B, 3]\n",
    "        if style_id.dim() == 1:\n",
    "            style_id = style_id.unsqueeze(1)\n",
    "        style_onehot = F.one_hot(style_id, num_classes=self.num_styles).float()  # [B, 1, 3]\n",
    "        style_onehot = style_onehot.squeeze(1)\n",
    "\n",
    "        # Expand to [B, 3, H, W]\n",
    "        B, _, H, W = x.shape\n",
    "        #style_map = style_onehot.unsqueeze(2).unsqueeze(3).expand(B, self.num_styles, H, W)\n",
    "        style_map = style_onehot.unsqueeze(2).unsqueeze(3).repeat(1, 1, H, W)\n",
    "\n",
    "        # Concatenate to input\n",
    "        x = torch.cat([x, style_map], dim=1)  # input becomes [B, 6, H, W]\n",
    "\n",
    "        # Standard UNet forward\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        mid = self.middle(self.pool3(enc3))\n",
    "        dec3 = self.decoder3(torch.cat([self.upconv3(mid), enc3], dim=1))\n",
    "        dec2 = self.decoder2(torch.cat([self.upconv2(dec3), enc2], dim=1))\n",
    "        dec1 = self.decoder1(torch.cat([self.upconv1(dec2), enc1], dim=1))\n",
    "        return dec1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bbe612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "Epoch [1/50], Loss: 0.0297306\n",
      "Epoch [2/50], Loss: 0.0197056\n",
      "Epoch [3/50], Loss: 0.0186904\n",
      "Epoch [4/50], Loss: 0.0184432\n",
      "Epoch [5/50], Loss: 0.0184920\n",
      "Epoch [6/50], Loss: 0.0184308\n",
      "Epoch [7/50], Loss: 0.0181614\n",
      "Epoch [8/50], Loss: 0.0181555\n",
      "Epoch [9/50], Loss: 0.0174354\n",
      "Epoch [10/50], Loss: 0.0156544\n",
      "Epoch [11/50], Loss: 0.0151087\n",
      "Epoch [12/50], Loss: 0.0147580\n",
      "Epoch [13/50], Loss: 0.0142678\n",
      "Epoch [14/50], Loss: 0.0139481\n",
      "Epoch [15/50], Loss: 0.0137170\n",
      "Epoch [16/50], Loss: 0.0134296\n",
      "Epoch [17/50], Loss: 0.0131833\n",
      "Epoch [18/50], Loss: 0.0126863\n",
      "Epoch [19/50], Loss: 0.0126434\n",
      "Epoch [20/50], Loss: 0.0122865\n",
      "Epoch [21/50], Loss: 0.0119951\n",
      "Epoch [22/50], Loss: 0.0116827\n",
      "Epoch [23/50], Loss: 0.0116268\n",
      "Epoch [24/50], Loss: 0.0113672\n",
      "Epoch [25/50], Loss: 0.0111814\n",
      "Epoch [26/50], Loss: 0.0108342\n",
      "Epoch [27/50], Loss: 0.0107045\n",
      "Epoch [28/50], Loss: 0.0104727\n",
      "Epoch [29/50], Loss: 0.0102284\n",
      "Epoch [30/50], Loss: 0.0099634\n",
      "Epoch [31/50], Loss: 0.0096565\n",
      "Epoch [32/50], Loss: 0.0093060\n",
      "Epoch [33/50], Loss: 0.0090740\n",
      "Epoch [34/50], Loss: 0.0086929\n",
      "Epoch [35/50], Loss: 0.0085031\n",
      "Epoch [36/50], Loss: 0.0082055\n",
      "Epoch [37/50], Loss: 0.0079168\n",
      "Epoch [38/50], Loss: 0.0076195\n",
      "Epoch [39/50], Loss: 0.0074901\n",
      "Epoch [40/50], Loss: 0.0071602\n",
      "Epoch [41/50], Loss: 0.0067814\n",
      "Epoch [42/50], Loss: 0.0065736\n",
      "Epoch [43/50], Loss: 0.0063045\n",
      "Epoch [44/50], Loss: 0.0061133\n",
      "Epoch [45/50], Loss: 0.0058369\n",
      "Epoch [46/50], Loss: 0.0056614\n",
      "Epoch [47/50], Loss: 0.0055114\n",
      "Epoch [48/50], Loss: 0.0053545\n",
      "Epoch [49/50], Loss: 0.0050894\n",
      "Epoch [50/50], Loss: 0.0049603\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device = torch.device('cuda' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "model = UNet().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.float32)\n",
    "        style_id = torch.full((images.size(0),), 2, dtype=torch.long, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, style_id)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(trainloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.7f}')\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'styled_unet_model.pth')\n",
    "print('Model saved as styled_unet_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "142057e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "model.eval()\n",
    "\n",
    "output_folder = './test'\n",
    "input_folder = './dataset/Comic Style/train/input' \n",
    "\n",
    "image_paths = []\n",
    "for f in os.listdir(input_folder):\n",
    "    path = os.path.join(input_folder, f)\n",
    "    image_paths.append(path)\n",
    "\n",
    "selected_paths = random.sample(image_paths, 50)\n",
    "\n",
    "for i, path in enumerate(selected_paths):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    size = img.size\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)  # [1, 3, H, W]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor, style_id)\n",
    "        #output = output.clamp(0, 1)  # Ensure pixel range [0, 1]\n",
    "        output_resized = F.interpolate(output, size=size[::-1], mode='bilinear', align_corners=False)\n",
    "\n",
    "    save_image(output_resized, os.path.join(output_folder, f\"styled_3d_{i+1}.jpg\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
